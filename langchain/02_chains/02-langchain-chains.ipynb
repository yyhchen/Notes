{"cells":[{"cell_type":"code","execution_count":null,"id":"uZR3iGJJtdDE","metadata":{"id":"uZR3iGJJtdDE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684335097489,"user_tz":-480,"elapsed":13346,"user":{"displayName":"Coleridge Constance","userId":"01658682311516132277"}},"outputId":"6115bcc3-21aa-468d-e2c9-48f7e9d97b45"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/846.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.5/846.5 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m846.5/846.5 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -qU langchain openai"]},{"cell_type":"markdown","id":"7a4ba72d","metadata":{"id":"7a4ba72d"},"source":["\n","# Getting Started with Chains\n","\n","Chains are the core of LangChain. They are simply a chain of components, executed in a particular order.\n","\n","The simplest of these chains is the `LLMChain`. It works by taking a user's input, passing in to the first element in the chain â€” a `PromptTemplate` â€” to format the input into a particular prompt. The formatted prompt is then passed to the next (and final) element in the chain â€” a LLM.\n","\n","We'll start by importing all the libraries that we'll be using in this example."]},{"cell_type":"code","execution_count":null,"id":"66fb9c2a","metadata":{"id":"66fb9c2a"},"outputs":[],"source":["import inspect\n","import re\n","\n","from getpass import getpass\n","from langchain import OpenAI, PromptTemplate\n","from langchain.chains import LLMChain, LLMMathChain, TransformChain, SequentialChain\n","from langchain.callbacks import get_openai_callback"]},{"cell_type":"markdown","id":"wPdWz1IdxyBR","metadata":{"id":"wPdWz1IdxyBR"},"source":["To run this notebook, we will need to use an OpenAI LLM. Here we will setup the LLM we will use for the whole notebook, just input your openai api key when prompted."]},{"cell_type":"code","execution_count":null,"id":"v86cmyppxdfc","metadata":{"id":"v86cmyppxdfc"},"outputs":[],"source":["import os\n","\n","os.environ['OPENAI_API_KEY'] = ''"]},{"cell_type":"code","execution_count":null,"id":"baaa74b8","metadata":{"id":"baaa74b8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684335417730,"user_tz":-480,"elapsed":925,"user":{"displayName":"Coleridge Constance","userId":"01658682311516132277"}},"outputId":"e70a2ec7-5d90-4080-85a6-08da6cfb3fa3"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/langchain/llms/openai.py:169: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/langchain/llms/openai.py:696: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n","  warnings.warn(\n"]}],"source":["llm = OpenAI(\n","    model_name=\"gpt-3.5-turbo\",\n","    openai_api_key=''\n","    )"]},{"cell_type":"markdown","id":"309g_2pqxzzB","metadata":{"id":"309g_2pqxzzB"},"source":["An extra utility we will use is this function that will tell us how many tokens we are using in each call. This is a good practice that is increasingly important as we use more complex tools that might make several calls to the API (like agents). It is very important to have a close control of how many tokens we are spending to avoid unsuspected expenditures."]},{"cell_type":"code","execution_count":null,"id":"DsC3szr6yP3L","metadata":{"id":"DsC3szr6yP3L"},"outputs":[],"source":["def count_tokens(chain, query):\n","    with get_openai_callback() as cb:\n","        result = chain.run(query)\n","        print(f'Spent a total of {cb.total_tokens} tokens')\n","\n","    return result"]},{"cell_type":"markdown","id":"6e1f31b4","metadata":{"id":"6e1f31b4"},"source":["## What are chains anyway?"]},{"cell_type":"markdown","id":"5b919c3a","metadata":{"id":"5b919c3a"},"source":["**Definition**: Chains are one of the fundamental building blocks of this lib (as you can guess!).\n","\n","The official definition of chains is the following:\n","\n","\n","> A chain is made up of links, which can be either primitives or other chains. Primitives can be either prompts, llms, utils, or other chains.\n","\n","\n","So a chain is basically a pipeline that processes an input by using a specific combination of primitives. Intuitively, it can be thought of as a 'step' that performs a certain set of operations on an input and returns the result. They can be anything from a prompt-based pass through a LLM to applying a Python function to an text."]},{"cell_type":"markdown","id":"c4644b2f","metadata":{"id":"c4644b2f"},"source":["Chains are divided in three types: Utility chains, Generic chains and Combine Documents chains. In this edition, we will focus on the first two since the third is too specific (will be covered in due course).\n","\n","1. Utility Chains: chains that are usually used to extract a specific answer from a llm with a very narrow purpose and are ready to be used out of the box.\n","2. Generic Chains: chains that are used as building blocks for other chains but cannot be used out of the box on their own."]},{"cell_type":"markdown","id":"e4d283b6","metadata":{"id":"e4d283b6"},"source":["Let's take a peek into what these chains have to offer!"]},{"cell_type":"markdown","id":"831827b7","metadata":{"id":"831827b7"},"source":["### Utility Chains"]},{"cell_type":"markdown","id":"6c66e4b4","metadata":{"id":"6c66e4b4"},"source":["Let's start with a simple utility chain. The `LLMMathChain` gives llms the ability to do math. Let's see how it works!"]},{"cell_type":"markdown","id":"HF3XCWD2sVi0","metadata":{"id":"HF3XCWD2sVi0"},"source":["#### Pro-tip: use `verbose=True` to see what the different steps in the chain are!"]},{"cell_type":"code","execution_count":null,"id":"b4161561","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":226},"id":"b4161561","outputId":"99da75cc-af83-4e86-8173-6bda4aff1a22","executionInfo":{"status":"ok","timestamp":1684336319645,"user_tz":-480,"elapsed":4192,"user":{"displayName":"Coleridge Constance","userId":"01658682311516132277"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n","What is 13 raised to the 0.587965 power?\u001b[32;1m\u001b[1;3m```text\n","13 ** 0.587965\n","```\n","...numexpr.evaluate(\"13 ** 0.587965\")...\n","\u001b[0m\n","Answer: \u001b[33;1m\u001b[1;3m4.51814019984498\u001b[0m\n","\u001b[1m> Finished chain.\u001b[0m\n","Spent a total of 181 tokens\n"]},{"output_type":"execute_result","data":{"text/plain":["'Answer: 4.51814019984498'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":12}],"source":["llm_math = LLMMathChain(llm=llm, verbose=True)\n","\n","count_tokens(llm_math, \"What is 13 raised to the 0.587965 power?\")"]},{"cell_type":"markdown","id":"198eebb2","metadata":{"id":"198eebb2"},"source":["Let's see what is going on here. The chain recieved a question in natural language and sent it to the llm. The llm returned a Python code which the chain compiled to give us an answer. A few questions arise.. How did the llm know that we wanted it to return Python code?"]},{"cell_type":"markdown","id":"a7a0821a","metadata":{"id":"a7a0821a"},"source":["**Enter prompts**"]},{"cell_type":"markdown","id":"c86c5798","metadata":{"id":"c86c5798"},"source":["The question we send as input to the chain is not the only input that the llm recieves ðŸ˜‰. The input is inserted into a wider context, which gives precise instructions on how to interpret the input we send. This is called a _prompt_. Let's see what this chain's prompt is!"]},{"cell_type":"code","execution_count":null,"id":"62778ef4","metadata":{"id":"62778ef4","outputId":"86a9e781-289d-47ea-9c5b-887dfe3cd066","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684336381397,"user_tz":-480,"elapsed":377,"user":{"displayName":"Coleridge Constance","userId":"01658682311516132277"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n","\n","Question: ${{Question with math problem.}}\n","```text\n","${{single line mathematical expression that solves the problem}}\n","```\n","...numexpr.evaluate(text)...\n","```output\n","${{Output of running the code}}\n","```\n","Answer: ${{Answer}}\n","\n","Begin.\n","\n","Question: What is 37593 * 67?\n","\n","```text\n","37593 * 67\n","```\n","...numexpr.evaluate(\"37593 * 67\")...\n","```output\n","2518731\n","```\n","Answer: 2518731\n","\n","Question: {question}\n","\n"]}],"source":["print(llm_math.prompt.template)"]},{"cell_type":"markdown","id":"708031d8","metadata":{"id":"708031d8"},"source":["Ok.. let's see what we got here. So, we are literally telling the llm that for complex math problems **it should not try to do math on its own** but rather it should print a Python code that will calculate the math problem instead. Probably, if we just sent the query without any context, the llm would try (and fail) to calculate this on its own. Wait! This is testable.. let's try it out! ðŸ§"]},{"cell_type":"code","execution_count":null,"id":"66b92768","metadata":{"id":"66b92768","outputId":"c3d01a04-fb5f-4018-dfc0-6c6c42cb2394","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1684336040976,"user_tz":-480,"elapsed":2000,"user":{"displayName":"Coleridge Constance","userId":"01658682311516132277"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Spent a total of 28 tokens\n"]},{"output_type":"execute_result","data":{"text/plain":["'Answer: 4.256'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}],"source":["# we set the prompt to only have the question we ask\n","prompt = PromptTemplate(input_variables=['question'], template='{question}')\n","llm_chain = LLMChain(prompt=prompt, llm=llm)\n","\n","# we ask the llm for the answer with no context\n","\n","count_tokens(llm_chain, \"What is 13 raised to the 0.587965 power?\")"]},{"cell_type":"markdown","id":"d147e7bf","metadata":{"id":"d147e7bf"},"source":["Wrong answer! Herein lies the power of prompting and one of our most important insights so far:\n","\n","**Insight**: _by using prompts intelligently, we can force the llm to avoid common pitfalls by explicitly and purposefully programming it to behave in a certain way._"]},{"cell_type":"markdown","id":"1cd2a31f","metadata":{"id":"1cd2a31f"},"source":["Another interesting point about this chain is that it not only runs an input through the llm but it later compiles Python code. Let's see exactly how this works."]},{"cell_type":"code","execution_count":null,"id":"3488c5b6","metadata":{"id":"3488c5b6","outputId":"8b32a998-8e11-48bf-f21c-f5d086186508"},"outputs":[{"name":"stdout","output_type":"stream","text":["    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n","        llm_executor = LLMChain(prompt=self.prompt, llm=self.llm)\n","        python_executor = PythonREPL()\n","        self.callback_manager.on_text(inputs[self.input_key], verbose=self.verbose)\n","        t = llm_executor.predict(question=inputs[self.input_key], stop=[\"```output\"])\n","        self.callback_manager.on_text(t, color=\"green\", verbose=self.verbose)\n","        t = t.strip()\n","        if t.startswith(\"```python\"):\n","            code = t[9:-4]\n","            output = python_executor.run(code)\n","            self.callback_manager.on_text(\"\\nAnswer: \", verbose=self.verbose)\n","            self.callback_manager.on_text(output, color=\"yellow\", verbose=self.verbose)\n","            answer = \"Answer: \" + output\n","        elif t.startswith(\"Answer:\"):\n","            answer = t\n","        else:\n","            raise ValueError(f\"unknown format from LLM: {t}\")\n","        return {self.output_key: answer}\n","\n"]}],"source":["print(inspect.getsource(llm_math._call))"]},{"cell_type":"markdown","id":"fa6b6c2e","metadata":{"id":"fa6b6c2e"},"source":["So we can see here that if the llm returns Python code we will compile it with a Python REPL* simulator. We now have the full picture of the chain: either the llm returns an answer (for simple math problems) or it returns Python code which we compile for an exact answer to harder problems. Smart!"]},{"cell_type":"markdown","id":"67f96bd3","metadata":{"id":"67f96bd3"},"source":["Also notice that here we get our first example of **chain composition**, a key concept behind what makes langchain special. We are using the `LLMMathChain` which in turn initializes and uses an `LLMChain` (a 'Generic Chain') when called. We can make any arbitrary number of such compositions, effectively 'chaining' many such chains to achieve highly complex and customizable behaviour."]},{"cell_type":"markdown","id":"b109619a","metadata":{"id":"b109619a"},"source":["Utility chains usually follow this same basic structure: there is a prompt for constraining the llm to return a very specific type of response from a given query. We can ask the llm to create SQL queries, API calls and even create Bash commands on the fly ðŸ”¥\n","\n","The list continues to grow as langchain becomes more and more flexible and powerful so we encourage you to [check it out](https://langchain.readthedocs.io/en/latest/modules/chains/utility_how_to.html) and tinker with the example notebooks that you might find interesting."]},{"cell_type":"markdown","id":"381e329c","metadata":{"id":"381e329c"},"source":["*_A Python REPL (Read-Eval-Print Loop) is an interactive shell for executing Python code line by line_"]},{"cell_type":"markdown","id":"f66a25a2","metadata":{"id":"f66a25a2"},"source":["### Generic chains"]},{"cell_type":"markdown","id":"70b32a84","metadata":{"id":"70b32a84"},"source":["There are only three Generic Chains in langchain and we will go all in to showcase them all in the same example. Let's go!"]},{"cell_type":"markdown","id":"4b8e2048","metadata":{"id":"4b8e2048"},"source":["Say we have had experience of getting dirty input texts. Specifically, as we know, llms charge us by the number of tokens we use and we are not happy to pay extra when the input has extra characters. Plus its not neat ðŸ˜‰"]},{"cell_type":"markdown","id":"a6e778d2","metadata":{"id":"a6e778d2"},"source":["First, we will build a custom transform function to clean the spacing of our texts. We will then use this function to build a chain where we input our text and we expect a clean text as output."]},{"cell_type":"code","execution_count":null,"id":"c794e00a","metadata":{"id":"c794e00a"},"outputs":[],"source":["def transform_func(inputs: dict) -> dict:\n","    text = inputs[\"text\"]\n","\n","    # replace multiple new lines and multiple spaces with a single one\n","    text = re.sub(r'(\\r\\n|\\r|\\n){2,}', r'\\n', text)\n","    text = re.sub(r'[ \\t]+', ' ', text)\n","\n","    return {\"output_text\": text}"]},{"cell_type":"markdown","id":"42dc1ac6","metadata":{"id":"42dc1ac6"},"source":["Importantly, when we initialize the chain we do not send an llm as an argument. As you can imagine, not having an llm makes this chain's abilities much weaker than the example we saw earlier. However, as we will see next, combining this chain with other chains can give us highly desirable results."]},{"cell_type":"code","execution_count":null,"id":"286f7295","metadata":{"id":"286f7295"},"outputs":[],"source":["clean_extra_spaces_chain = TransformChain(input_variables=[\"text\"], output_variables=[\"output_text\"], transform=transform_func)"]},{"cell_type":"code","execution_count":null,"id":"977bf11a","metadata":{"id":"977bf11a","outputId":"4c46f971-fed0-4f36-c866-bf162329797e","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1684336770316,"user_tz":-480,"elapsed":735,"user":{"displayName":"Coleridge Constance","userId":"01658682311516132277"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'A random text with some irregular spacing.\\n Another one here as well.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}],"source":["clean_extra_spaces_chain.run('A random text  with   some irregular spacing.\\n\\n\\n     Another one   here as well.')"]},{"cell_type":"markdown","id":"b3f84cd0","metadata":{"id":"b3f84cd0"},"source":["Great! Now things will get interesting.\n","\n","Say we want to use our chain to clean an input text and then paraphrase the input in a specific style, say a poet or a policeman. As we now know, the `TransformChain` does not use a llm so the styling will have to be done elsewhere. That's where our `LLMChain` comes in. We know about this chain already and we know that we can do cool things with smart prompting so let's take a chance!"]},{"cell_type":"markdown","id":"5b77042a","metadata":{"id":"5b77042a"},"source":["First we will build the prompt template:"]},{"cell_type":"code","execution_count":null,"id":"73719a5d","metadata":{"id":"73719a5d"},"outputs":[],"source":["template = \"\"\"Paraphrase this text:\n","\n","{output_text}\n","\n","In the style of a {style}.\n","\n","Paraphrase: \"\"\"\n","prompt = PromptTemplate(input_variables=[\"style\", \"output_text\"], template=template)"]},{"cell_type":"markdown","id":"83b2ec83","metadata":{"id":"83b2ec83"},"source":["And next, initialize our chain:"]},{"cell_type":"code","execution_count":null,"id":"48a067ab","metadata":{"id":"48a067ab"},"outputs":[],"source":["style_paraphrase_chain = LLMChain(llm=llm, prompt=prompt, output_key='final_output')"]},{"cell_type":"markdown","id":"2324005d","metadata":{"id":"2324005d"},"source":["Great! Notice that the input text in the template is called 'output_text'. Can you guess why?\n","\n","We are going to pass the output of the `TransformChain` to the `LLMChain`!"]},{"cell_type":"markdown","id":"c5da4925","metadata":{"id":"c5da4925"},"source":["Finally, we need to combine them both to work as one integrated chain. For that we will use `SequentialChain` which is our third generic chain building block."]},{"cell_type":"code","execution_count":null,"id":"06f51f17","metadata":{"id":"06f51f17"},"outputs":[],"source":["sequential_chain = SequentialChain(chains=[clean_extra_spaces_chain, style_paraphrase_chain], input_variables=['text', 'style'], output_variables=['final_output'])"]},{"cell_type":"markdown","id":"7f0f51d8","metadata":{"id":"7f0f51d8"},"source":["Our input is the langchain docs description of what chains are but dirty with some extra spaces all around."]},{"cell_type":"code","execution_count":null,"id":"a8032489","metadata":{"id":"a8032489"},"outputs":[],"source":["input_text = \"\"\"\n","Chains allow us to combine multiple\n","\n","\n","components together to create a single, coherent application.\n","\n","For example, we can create a chain that takes user input,       format it with a PromptTemplate,\n","\n","and then passes the formatted response to an LLM. We can build more complex chains by combining     multiple chains together, or by\n","\n","\n","combining chains with other components.\n","\"\"\""]},{"cell_type":"markdown","id":"b2f55d21","metadata":{"id":"b2f55d21"},"source":["We are all set. Time to get creative!"]},{"cell_type":"code","execution_count":null,"id":"d507aa5c","metadata":{"id":"d507aa5c","outputId":"f51b71c2-79bb-4d12-eefb-fc471b9c0b8c","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1684336903134,"user_tz":-480,"elapsed":9384,"user":{"displayName":"Coleridge Constance","userId":"01658682311516132277"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Spent a total of 175 tokens\n"]},{"output_type":"execute_result","data":{"text/plain":["\"Yo, check it, \\nchains help us build apps that are sick. \\nWith 'em, we can mix and match, \\nand take user input from scratch. \\nWe use a PromptTemplate for format, \\nthen the LLM takes care of the rest. \\nWe could even combine chains, \\nor partner with other components to elevate the game.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":21}],"source":["count_tokens(sequential_chain, {'text': input_text, 'style': 'a 90s rapper'})"]},{"cell_type":"markdown","id":"60b52e19","metadata":{"id":"60b52e19"},"source":["## A note on langchain-hub"]},{"cell_type":"markdown","id":"02f649da","metadata":{"id":"02f649da"},"source":["`langchain-hub` is a sister library to `langchain`, where all the chains, agents and prompts are serialized for us to use."]},{"cell_type":"code","execution_count":null,"id":"411500c2","metadata":{"id":"411500c2"},"outputs":[],"source":["from langchain.chains import load_chain"]},{"cell_type":"markdown","id":"b375e5b7","metadata":{"id":"b375e5b7"},"source":["Loading from langchain hub is as easy as finding the chain you want to load in the repository and then using `load_chain` with the corresponding path. We also have `load_prompt` and `initialize_agent`, but more on that later. Let's see how we can do this with our `LLMMathChain` we saw earlier:"]},{"cell_type":"code","execution_count":null,"id":"fbe8748d","metadata":{"id":"fbe8748d"},"outputs":[],"source":["llm_math_chain = load_chain('lc://chains/llm-math/chain.json')"]},{"cell_type":"markdown","id":"ebcfe67c","metadata":{"id":"ebcfe67c"},"source":["What if we want to change some of the configuration parameters? We can simply override it after loading:"]},{"cell_type":"code","execution_count":null,"id":"d0d54233","metadata":{"id":"d0d54233","outputId":"92eba1cf-e47b-4df1-cc51-caee5a6a720b"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["llm_math_chain.verbose"]},{"cell_type":"code","execution_count":null,"id":"074f8806","metadata":{"id":"074f8806"},"outputs":[],"source":["llm_math_chain.verbose = False"]},{"cell_type":"code","execution_count":null,"id":"465a6cbf","metadata":{"id":"465a6cbf","outputId":"0207bf08-0db0-4d85-e3b9-ac7922f344c4"},"outputs":[{"data":{"text/plain":["False"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["llm_math_chain.verbose"]},{"cell_type":"markdown","id":"2cc688ca","metadata":{"id":"2cc688ca"},"source":["That's it for this example on chains.\n","\n","---"]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/pinecone-io/examples/blob/master/generation/langchain/handbook/02-langchain-chains.ipynb","timestamp":1684292744686}]},"kernelspec":{"display_name":"ml","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"b8e7999f96e1b425e2d542f21b571f5a4be3e97158b0b46ea1b2500df63956ce"}}},"nbformat":4,"nbformat_minor":5}