# Asymmetric Quantization

---

> 常见的 [AWQ](/NLP%20review/model%20deploy%20and%20accumulate/model_compression/Quantization/AWQ.md), [GPTQ](/NLP%20review/model%20deploy%20and%20accumulate/model_compression/Quantization/GPTQ.md) 等量化方法，可以是对称，也可以是非对称的~

<br>

**非对称量化**（Asymmetric Quantization）是深度学习模型量化中的一种方法，它与对称量化（Symmetric Quantization）相对。在非对称量化中，权重或激活的量化参数（例如比例因子和零点）被独立选择，以便更精确地匹配数据的实际分布。这种方法的一个关键特点是零点（zero-point）不为零，这允许更灵活地表示正数和负数的值，从而减少量化误差。

在对称量化中，零点通常被设置为零，这意味着量化的范围是围绕零点对称的。这种方法简化了计算，因为不需要处理零点偏移，但它限制了整数和浮点数域之间的映射。结果，对称量化更适合于像ReLU激活这样的单尾分布，而对于围绕零对称分布的数据，可能不太适用。

非对称量化通过引入一个非零的零点，提供了额外的灵活性，允许更精细地控制量化过程。例如，比例因子（scale factor）和零点（zero-point）被用来将浮点值映射到整数网格，其中零点确保真实零值可以无误差地量化。这对于确保像零填充或ReLU这样的常见操作不引入量化误差非常重要。

在实际应用中，非对称量化可以提供比对称量化更好的性能，尤其是在量化权重和激活时。然而，这种额外的灵活性可能会带来一些计算开销，因为需要处理额外的数据依赖项。例如，当使用非对称量化的权重和激活进行乘法操作时，可能需要计算额外的项来处理输入数据 。

总的来说，非对称量化是一种在深度学习模型量化中广泛使用的技术，它通过独立选择量化参数来减少量化误差，尽管这可能会增加一些计算复杂性。
