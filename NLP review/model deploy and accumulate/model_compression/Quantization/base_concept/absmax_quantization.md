# absolute maximum (absmax) quantization

---


在深度学习中，"absolute maximum (absmax) quantization" 是一种量化技术，用于将浮点数（通常是32位浮点数）转换为较低精度的整数表示，以减少模型的存储空间和计算复杂度。这种技术通常用于模型压缩和加速推理过程。

### 基本概念

1. **量化**：量化是将高精度的数值（如32位浮点数）转换为低精度的数值（如8位整数）的过程。这可以显著减少模型的存储需求和计算量。

2. **绝对最大值（absmax）**：在absmax量化中，量化范围是根据输入数据的最大绝对值来确定的。具体来说，输入数据的最大绝对值（即 `max(abs(x))`）被用作量化的参考点。

### 量化过程

假设我们有一个浮点数张量 `x`，我们希望将其量化为8位整数。absmax量化的步骤如下：

1. **找到最大绝对值**：计算张量 `x` 中的最大绝对值 `max_abs = max(abs(x))`。

2. **缩放因子**：确定一个缩放因子 `scale`，通常是 `scale = max_abs / 127`。这里127是因为我们使用8位有符号整数表示，其范围是 `-128` 到 `127`。

3. **量化**：将每个浮点数 `x_i` 量化为整数 `q_i`，公式为：
   \[
   q_i = \text{round}(x_i / \text{scale})
   \]
   这里 `round` 表示四舍五入。

4. **反量化**：在推理过程中，如果需要将量化后的整数转换回浮点数，可以使用反量化公式：
   \[
   x_i' = q_i \times \text{scale}
   \]

### 示例

假设我们有一个浮点数张量 `x`：

```python
x = [0.1, -0.3, 0.5, -0.7, 0.9]
```

1. **找到最大绝对值**：
   \[
   max_abs = max(abs(0.1), abs(-0.3), abs(0.5), abs(-0.7), abs(0.9)) = 0.9
   \]

2. **缩放因子**：
   \[
   scale = 0.9 / 127 \approx 0.0071
   \]

3. **量化**：
   \[
   q_0 = \text{round}(0.1 / 0.0071) \approx \text{round}(14.08) = 14
   \]
   \[
   q_1 = \text{round}(-0.3 / 0.0071) \approx \text{round}(-42.25) = -42
   \]
   \[
   q_2 = \text{round}(0.5 / 0.0071) \approx \text{round}(70.42) = 70
   \]
   \[
   q_3 = \text{round}(-0.7 / 0.0071) \approx \text{round}(-98.59) = -99
   \]
   \[
   q_4 = \text{round}(0.9 / 0.0071) \approx \text{round}(126.76) = 127
   \]

   量化后的整数张量为：
   \[
   q = [14, -42, 70, -99, 127]
   \]

4. **反量化**：
   \[
   x_0' = 14 \times 0.0071 \approx 0.1
   \]
   \[
   x_1' = -42 \times 0.0071 \approx -0.3
   \]
   \[
   x_2' = 70 \times 0.0071 \approx 0.5
   \]
   \[
   x_3' = -99 \times 0.0071 \approx -0.7
   \]
   \[
   x_4' = 127 \times 0.0071 \approx 0.9
   \]

   反量化后的浮点数张量为：
   \[
   x' = [0.1, -0.3, 0.5, -0.7, 0.9]
   \]

### 总结

通过absmax量化，我们将原始的浮点数张量转换为8位整数表示，从而减少了存储空间和计算量。在推理过程中，我们可以通过反量化将这些整数转换回浮点数，以恢复原始的数值。这种技术在模型压缩和加速推理中非常有用。