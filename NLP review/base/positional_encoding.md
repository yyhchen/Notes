# 几种位置编码

---

### transformer原生的绝对位置编码




<br>
<br>



### RoPE

[RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)

和相对位置编码相比，**RoPE 具有更好的外推性**，目前是大模型相对位置编码中应用最广的方式之一。

<br>

**注：什么是大模型外推性？**

外推性是**指大模型在训练时和预测时的输入长度不一致，导致模型的泛化能力下降的问题**。例如，如果一个模型在训练时只使用了512个 token 的文本，那么在预测时如果输入超过512个 token，模型可能无法正确处理。这就限制了大模型在处理长文本或多轮对话等任务时的效果。

而RoPE能够提升大模型的外推能力（2k推8k，甚至更长）


[十分钟读懂旋转编码（RoPE）](https://www.zhihu.com/tardis/zm/art/647109286?source_id=1003)




<br>



### 旋转位置编码 和 绝对位置编码 区别

旋转位置编码（Rotary Positional Encoding）和绝对位置编码（Absolute Positional Encoding）都是用于在Transformer模型中引入位置信息的策略，但它们的工作方式和效果有所不同。
1. 绝对位置编码：
绝对位置编码是最早用于Transformer模型中的位置编码方法。它通常使用正弦和余弦函数来生成位置向量，这些向量与词嵌入向量相加，以引入序列中每个词的位置信息。绝对位置编码是固定的，不依赖于模型的其他参数，因此在训练过程中是固定的，不会随着模型的学习而改变。
绝对位置编码的一个例子是使用正弦和余弦函数的sinusoidal位置编码，其公式如下：
\[ \text{PosEnc}(pos, 2i) = \sin(pos / 10000^{2i/d_{\text{model}}}) \]
\[ \text{PosEnc}(pos, 2i+1) = \cos(pos / 10000^{2i/d_{\text{model}}}) \]
其中，\( pos \) 是位置索引，\( i \) 是维度索引，\( d_{\text{model}} \) 是词嵌入的维度。
2. 旋转位置编码：
旋转位置编码是近期提出的一种位置编码方法，它通过旋转注意力矩阵中的查询（Q）和键（K）向量来引入位置信息。这种方法不是将位置向量直接添加到词嵌入中，而是在计算注意力权重之前，通过旋转操作来使不同位置的查询和键向量产生差异。

旋转位置编码的一个例子是使用复数表示的位置编码，其中查询和键向量的实部和虚部分别代表位置信息。通过在计算注意力权重之前对查询和键向量进行旋转，可以确保模型能够捕捉到序列中的位置关系。

旋转位置编码的主要优点是它能够更好地处理长序列，因为在长序列中，绝对位置编码可能会遇到位置表示的模糊性问题。此外，旋转位置编码还能够自然地扩展到超出训练时见过的最大序列长度。

总的来说，绝对位置编码是通过添加固定的位置向量来引入位置信息，而旋转位置编码是通过在注意力机制中应用旋转操作来引入位置信息。旋转位置编码在处理长序列时可能更有优势，而绝对位置编码则是一种简单且广泛使用的方法。
