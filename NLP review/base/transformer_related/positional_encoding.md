# 几种位置编码

---

## transformer原生的绝对位置编码



### 1. **Sinusoidal位置编码的原理**
在Transformer中，绝对位置编码的公式为：

$$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right), \quad PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right)$$
其中：
- `pos` 是绝对位置（如序列中的第5个词）。
- `i` 是维度索引（如嵌入向量的第0维、第1维等）。
- `d` 是嵌入维度。

这个编码的特点是 **周期性的**：不同位置通过不同频率的正弦/余弦函数组合，理论上可以编码任意长度的位置。

---

### 2. **相对位置编码的简化版**
假设我们用 **相对距离** \(d = |pos_i - pos_j|\) 替代绝对位置，并用正弦函数编码相对距离：

$$RE_{(d, 2i)} = \sin\left(\frac{d}{10000^{2i/d}}\right), \quad RE_{(d, 2i+1)} = \cos\left(\frac{d}{10000^{2i/d}}\right)$$

模型在训练时，只见过较短的相对距离（例如 $d \leq 100$）。

---

### 3. **示例：短距离 vs 长距离**
#### 场景1：训练时的短距离（d=5）
假设模型在训练时见过 \(d=5\)：

$$RE_{(5, 0)} = \sin\left(\frac{5}{10000^{0/4}}\right) = \sin(5)$$

此时模型可以学习到 \(d=5\) 对应的位置关系（例如相邻词的依赖）。

---

#### 场景2：测试时的长距离（d=105）
当测试时遇到 \(d=105\)（比训练时的最大距离100更远）：

$$RE_{(105, 0)} = \sin\left(\frac{105}{10000^{0/4}}\right) = \sin(105)$$

问题来了：
- **周期性干扰**：$\sin(105)$ 和 $\sin(105 - 16\pi) \approx \sin(105 - 50.27) = \sin(54.73)$ 值接近，导致模型可能将 $d=105$ 误判为 $d \approx 55$。
- **外推失效**：模型从未见过 $d=105$ 的编码，无法理解这是一个“更远”的距离。

---

### 4. **数学视角：为什么长距离会失效？**
#### （1）高频维度振荡过快
对于维度 \(i\) 较大的位置（高频维度），分母 $10000^{2i/d}$ 很小，导致频率极高：

$$\frac{d}{10000^{2i/d}} \rightarrow \text{非常大的输入值}$$

例如，若 \(i=3\), \(d=1024\)，则：

$$\frac{1024}{10000^{6/4}} \approx \frac{1024}{1000^{1.5}} \approx \frac{1024}{31623} \approx 0.032$$

此时 $$\sin(0.032) \approx 0.032$$，几乎无区分度。

#### （2）低频维度过于平滑
对于维度 \(i\) 较小的位置（低频维度），频率较低，但长距离的差异被压缩：

$$\frac{105}{10000^{0/4}} = 105 \quad (\sin(105) \approx -0.97)$$

而 \(d=55\) 时：

$$\frac{55}{10000^{0/4}} = 55 \quad (\sin(55) \approx -0.99)$$

两者数值接近，模型难以区分 \(d=55\) 和 \(d=105\)。

---

### 5. **具体案例：长序列中的错误**
假设任务是从序列中预测被遮盖的词：
- **训练序列（长度50）**：`A B [MASK] D E ... (共50个词)`  
  模型学习到 `[MASK]`（位置3）应关注位置2（`B`, \(d=-1\)）和位置4（`D`, \(d=+1\)）。

- **测试序列（长度200）**：`A B [MASK] D E ... (共200个词)`  
  模型需要预测位置3的词，但此时：
  - 位置3到位置200的相对距离 \(d=197\)。
  - 根据正弦编码，\(d=197\) 可能与某个 \(d=97\) 的编码相似（因为 $$\sin(197) \approx \sin(197 - 3\pi) \approx \sin(197-9.42) = \sin(187.58)$$，继续周期性缩减……最终接近某个小距离的编码）。
  - 模型可能错误地关注到无关位置（如位置100），而不是附近的 `B` 或 `D`。

---

### 6. **关键问题总结**
| 问题类型       | 数学表现                             | 实际影响            |
| ---------- | -------------------------------- | --------------- |
| **周期性重复**  | $\sin(d)$ 和 $\sin(d + 2k\pi)$ 接近 | 模型混淆长距离和短距离     |
| **高频维度失效** | 极大距离导致高频维度值接近零                   | 长距离位置编码失去区分度    |
| **低频维度饱和** | 极大距离在低频维度压缩到相似值                  | 模型无法感知“远近”的细微差异 |

---

### 7. **改进方向**
- **局部窗口**：限制注意力范围（如只关注前后50个词），避免长距离依赖。
- **线性缩放**：将距离 \(d\) 按比例缩小（如 $d_{\text{scaled}} = d / L_{\text{max}}$），但会损失精度。
- **随机化编码**：对长距离使用随机初始化编码，但需重新训练模型适应。

---

### 直观类比
想象你用一把 **弹簧尺** 测量距离：  
- 短距离（<10米）时，弹簧伸缩正常，刻度清晰。  
- 长距离（>100米）时，弹簧被拉到极限，刻度挤在一起，无法分辨1米和2米的差异——这就是正弦编码在长序列中的困境。



<br>
<br>



## RoPE

[RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)

和相对位置编码相比，**RoPE 具有更好的外推性**，目前是大模型相对位置编码中应用最广的方式之一。

<br>

**注：什么是大模型外推性？**

外推性是**指大模型在训练时和预测时的输入长度不一致，导致模型的泛化能力下降的问题**。例如，如果一个模型在训练时只使用了512个 token 的文本，那么在预测时如果输入超过512个 token，模型可能无法正确处理。这就限制了大模型在处理长文本或多轮对话等任务时的效果。

而RoPE能够提升大模型的外推能力（2k推8k，甚至更长）


[十分钟读懂旋转编码（RoPE）](https://www.zhihu.com/tardis/zm/art/647109286?source_id=1003)




<br>



## 旋转位置编码 和 绝对位置编码 区别

旋转位置编码（Rotary Positional Encoding）和绝对位置编码（Absolute Positional Encoding）都是用于在Transformer模型中引入位置信息的策略，但它们的工作方式和效果有所不同。
1. 绝对位置编码：
绝对位置编码是最早用于Transformer模型中的位置编码方法。它通常使用正弦和余弦函数来生成位置向量，这些向量与词嵌入向量相加，以引入序列中每个词的位置信息。绝对位置编码是固定的，不依赖于模型的其他参数，因此在训练过程中是固定的，不会随着模型的学习而改变。
绝对位置编码的一个例子是使用正弦和余弦函数的sinusoidal位置编码，其公式如下：
$$\text{PosEnc}(pos, 2i) = \sin(pos / 10000^{2i/d_{\text{model}}})$$ 
$$\text{PosEnc}(pos, 2i+1) = \cos(pos / 10000^{2i/d_{\text{model}}})$$ 
其中，\( pos \) 是位置索引，\( i \) 是维度索引， $d_{model}$  是词嵌入的维度。
2. 旋转位置编码：
旋转位置编码是近期提出的一种位置编码方法，它通过旋转注意力矩阵中的查询（Q）和键（K）向量来引入位置信息。这种方法不是将位置向量直接添加到词嵌入中，而是在计算注意力权重之前，通过旋转操作来使不同位置的查询和键向量产生差异。

旋转位置编码的一个例子是使用复数表示的位置编码，其中查询和键向量的实部和虚部分别代表位置信息。通过在计算注意力权重之前对查询和键向量进行旋转，可以确保模型能够捕捉到序列中的位置关系。

旋转位置编码的主要优点是它能够更好地处理长序列，因为在长序列中，绝对位置编码可能会遇到位置表示的模糊性问题。此外，旋转位置编码还能够自然地扩展到超出训练时见过的最大序列长度。

总的来说，绝对位置编码是通过添加固定的位置向量来引入位置信息，而旋转位置编码是通过在注意力机制中应用旋转操作来引入位置信息。旋转位置编码在处理长序列时可能更有优势，而绝对位置编码则是一种简单且广泛使用的方法。
