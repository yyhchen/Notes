# transformer 工作流

---

![transformer structure](/NLP%20review/assets/transformer_structure.png)

> transfomer 架构

让我们通过一个具体的例子来详细解释Transformer架构的工作流程。假设我们有一个简单的翻译任务：将英文句子 "I love cats" 翻译成法文 "J'aime les chats"。

<br>

### 1. 输入表示

#### 1.1 嵌入向量
首先，我们将输入句子 "I love cats" 转换为嵌入向量。假设我们使用一个预训练的词嵌入模型，每个词被转换为一个512维的向量。

- "I" -> [0.1, 0.2, ..., 0.512]
- "love" -> [0.3, 0.4, ..., 0.512]
- "cats" -> [0.5, 0.6, ..., 0.512]

#### 1.2 位置编码
接下来，我们为每个词添加位置编码。假设我们使用正弦和余弦函数来生成位置编码，每个位置编码也是一个512维的向量。

- "I" (位置1) -> [sin(1), cos(1), ..., sin(512), cos(512)]
- "love" (位置2) -> [sin(2), cos(2), ..., sin(512), cos(512)]
- "cats" (位置3) -> [sin(3), cos(3), ..., sin(512), cos(512)]

将嵌入向量和位置编码相加，得到最终的输入表示：

- "I" -> [0.1 + sin(1), 0.2 + cos(1), ..., 0.512 + cos(512)]
- "love" -> [0.3 + sin(2), 0.4 + cos(2), ..., 0.512 + cos(512)]
- "cats" -> [0.5 + sin(3), 0.6 + cos(3), ..., 0.512 + cos(512)]

<br>

<br>

### 2. 编码器（Encoder）

假设我们使用6层的编码器。

#### 2.1 多头自注意力机制
在每一层中，输入序列首先通过多头自注意力机制。假设我们使用8个头。

- **输入**：[0.1 + sin(1), 0.2 + cos(1), ..., 0.512 + cos(512)] 等。
- **计算**：每个头独立计算注意力权重，然后将所有头的输出拼接起来并通过一个线性层进行整合。
- **输出**：一个新的表示序列，每个位置的表示都包含了整个输入序列的信息。

#### 2.2 前馈神经网络
自注意力机制的输出通过一个全连接的前馈神经网络。

- **输入**：自注意力机制的输出。
- **计算**：每个位置的表示通过两个线性变换和一个ReLU激活函数。
- **输出**：一个新的表示序列。

#### 2.3 残差连接和层归一化
每个子层的输出都加上其输入，形成一个残差连接，然后经过层归一化。

- **残差连接**：输出 = 输入 + 子层输出。
- **层归一化**：残差连接的输出经过层归一化。

<br>

<br>

### 3. 解码器（Decoder）

假设我们使用6层的解码器。

#### 3.1 多头自注意力机制（带掩码）
在每一层中，解码器首先通过一个带掩码的多头自注意力机制。

- **输入**：==目标序列==的嵌入和位置编码。（**“J'aime les chats”**）
- **计算**：每个头独立计算注意力权重，掩码防止解码器在生成当前词时“看到”未来的词。
- **输出**：一个新的表示序列。

#### 3.2 编码器-解码器注意力机制
解码器的第二个子层是一个编码器-解码器注意力机制。

- **输入**：自注意力机制的输出和编码器的输出。
- **计算**：解码器的查询（Query）来自解码器，键（Key）和值（Value）来自编码器。
- **输出**：一个新的表示序列。

#### 3.3 前馈神经网络
编码器-解码器注意力机制的输出通过一个全连接的前馈神经网络。

- **输入**：编码器-解码器注意力机制的输出。
- **计算**：每个位置的表示通过两个线性变换和一个ReLU激活函数。
- **输出**：一个新的表示序列。

#### 3.4 残差连接和层归一化
每个子层的输出都加上其输入，形成一个残差连接，然后经过层归一化。

- **残差连接**：输出 = 输入 + 子层输出。
- **层归一化**：残差连接的输出经过层归一化。

<br>

<br>

### 4. 输出层

解码器的最后一层的输出通过一个线性层和一个softmax层，生成最终的输出序列的概率分布。

- **输入**：解码器的最后一层的输出。
- **计算**：通过线性层和softmax层。
- **输出**：最终的输出序列 "J'aime les chats"。

<br>

<br>

### 总结

通过这个例子，我们可以看到Transformer架构的工作流程是如何一步步将输入序列 "I love cats" 转换为输出序列 "J'aime les chats" 的。编码器将输入序列转换为一系列高维表示，解码器则根据这些表示生成输出序列。整个架构通过自注意力机制和多头注意力机制，能够并行处理序列数据，并且在处理长距离依赖关系时表现出色。