## 在KL散度中，为什么是pxlog（px/qx）？而不是px·logpx，qx·logqx
KL散度（Kullback-Leibler Divergence）的公式是：
\[ D_{KL}(P||Q) = \sum_{i} P(i) \log\left(\frac{P(i)}{Q(i)}\right) \]
**这个公式中，\( P(i) \) 乘以 \( \log\left(\frac{P(i)}{Q(i)}\right) \) 而不是 \( P(i) \log(P(i)) \) 或 \( Q(i) \log(Q(i)) \) 的原因在于，KL散度的目的是衡量在给定的真实分布 \( P \) 下，使用另一种分布 \( Q \) 进行编码时，所需额外信息的平均位数。**
具体来说，\( P(i) \log(P(i)) \) 是分布 \( P \) 的熵，它衡量的是使用分布 \( P \) 对事件 \( i \) 进行编码时所需的最小平均比特数。同样，\( Q(i) \log(Q(i)) \) 是分布 \( Q \) 的熵，它衡量的是使用分布 \( Q \) 对事件 \( i \) 进行编码时所需的最小平均比特数。
**然而，KL散度不是简单地比较两个分布的熵，而是比较在真实分布 \( P \) 下，使用错误的分布 \( Q \) 进行编码时，相比于使用正确的分布 \( P \) 进行编码，所需额外信息的多少。** 因此，我们需要计算在分布 \( P \) 下，使用分布 \( Q \) 进行编码时，每个事件 \( i \) 的额外信息量，即 \( P(i) \log\left(\frac{P(i)}{Q(i)}\right) \)。
这个额外信息量是由两部分组成的：
1. \( P(i) \log(P(i)) \)：这是在分布 \( P \) 下，事件 \( i \) 的信息量。
2. \( -P(i) \log(Q(i)) \)：这是在分布 \( Q \) 下，事件 \( i \) 的信息量，但由于前面有负号，所以这里实际上是减去在分布 \( Q \) 下，事件 \( i \) 的信息量。
因此，KL散度可以看作是分布 \( P \) 的熵减去分布 \( Q \) 的交叉熵：
\[ D_{KL}(P||Q) = H(P) - H(P, Q) \]
其中，\( H(P) \) 是分布 \( P \) 的熵，\( H(P, Q) \) 是分布 \( P \) 和 \( Q \) 的交叉熵。这样，KL散度就提供了一种衡量两个概率分布之间差异的方法，特别是在我们关心一个特定分布 \( P \) 的情况下。
