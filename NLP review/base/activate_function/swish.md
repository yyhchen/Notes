Swish函数
---

<br>

### Swish函数的定义

Swish函数是一种自门控激活函数，由Google提出，其数学表达式为：

```
f(x) = x * sigmoid(βx)
```

其中，x是输入，β是一个可学习的参数，sigmoid是sigmoid函数。


如下图所示:

![swish](/NLP%20review/assets/swish.png)

<br>

### Swish函数的特点

* **自门控**：Swish函数通过sigmoid函数对输入进行门控，使得函数具有自适应性。
* **非单调性**：Swish函数是非单调的，这使其能够更好地拟合复杂函数。
* **平滑性**：Swish函数是连续可导的，这有助于优化算法的收敛。
* **上界无穷大**：Swish函数的上界是无穷大，这与ReLU类似，可以避免梯度饱和的问题。

<br>

### Swish函数的优势

* **性能优于ReLU**：在许多任务上，Swish函数表现出了比ReLU更好的性能，尤其是在深层网络中。
* **缓解梯度消失**：Swish函数在正向传播过程中能够产生较大的梯度，有助于缓解梯度消失问题。
* **提高模型的表达能力**：Swish函数的非单调性和平滑性使其能够更好地表达复杂的函数关系。

<br>


### Swish函数与ReLU函数的比较

| 特性        | ReLU                                    | Swish                                     |
| ----------- | -------------------------------------- | ---------------------------------------- |
| 负区间输出 | 0                                       | x * sigmoid(βx)                           |
| 连续性      | 不连续                                    | 连续                                      |
| 可导性      | 在x=0处不可导                            | 在x=0处可导                              |
| 非线性      | 线性                                     | 非线性                                     |
| 上界        | 无穷大                                     | 无穷大                                     |

<br>

### Swish函数的变体

* **Swish-B**：Swish-B是Swish函数的一个变体，其表达式为f(x) = x * sigmoid(bx)，其中b是一个常数。Swish-B在某些情况下可以取得更好的性能。

<br>

### 结论

Swish函数是一种新型的激活函数，其性能优于ReLU函数。Swish函数的自门控特性、非单调性和平滑性使其在深度学习中具有广泛的应用前景。



<br>
<br>



# 额外的

---

## 为什么说上界无限大可以避免梯度饱和？

首先。

### 梯度饱和是什么？

在神经网络的训练过程中，梯度饱和是指神经网络的某些层或整个网络的梯度变得非常小，甚至接近于零。这会导致网络的学习速度变慢，甚至停滞。

梯度饱和通常发生在激活函数的饱和区域。比如，sigmoid函数在输入很大或很小时，输出接近于0或1，导数接近于0，这就容易导致梯度饱和。

### 上界无限大如何避免梯度饱和？

* **激活函数的导数**：激活函数的导数表示神经元对输入的敏感程度。如果导数始终保持在一个较大的范围内，那么梯度就不会轻易变小，从而避免梯度饱和。
* **ReLU和Swish的对比**：
  * **ReLU**：当输入小于0时，输出为0，导数也为0。这使得ReLU在负半轴上容易出现梯度饱和。
  * **Swish**：Swish函数的上界是无穷大，这意味着即使输入非常大，输出也会继续增长，导数也不会趋近于0。这使得Swish函数在很大程度上避免了梯度饱和的问题。
* **直观理解**：
  * 我们可以将神经网络看作是一个多层的信息传递过程。如果每一层的激活函数都存在饱和问题，那么信息在传递过程中就会逐渐衰减，最终导致梯度消失。
  * 而上界无限大的激活函数，则可以保证信息在网络中能够有效地传递，从而避免梯度消失。

### 为什么Swish函数的上界是无穷大很重要？

* **保证信息传递**：上界无限大意味着激活函数的输出可以取任意大的值，这保证了信息能够在网络中有效地传递。
* **避免梯度消失**：上界无限大保证了激活函数的导数不会趋近于0，从而避免了梯度消失的问题。
* **提高网络的表达能力**：上界无限大使得网络能够更好地拟合复杂的数据。

**总结**

* 上界无限大的激活函数，如ReLU和Swish，可以有效地避免梯度饱和问题。
* 这些激活函数能够保证信息在网络中有效地传递，提高网络的学习能力。
* Swish函数相对于ReLU，具有更好的性能，因为它不仅避免了梯度饱和，还具有自门控和非单调性的特点，使得网络能够更好地拟合复杂函数。

**需要注意的是**，虽然上界无限大可以避免梯度饱和，但并不是所有上界无限大的激活函数都一定比其他激活函数好。不同的激活函数在不同的任务和网络结构中表现会有所不同。

