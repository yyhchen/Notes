# GELU函数


GELU（Gaussian Error Linear Unit）是一种激活函数，全称是高斯误差线性单元。它在很多方面都优于传统的激活函数，如ReLU，被广泛应用于各种神经网络模型中，尤其是Transformer模型。

**from paper**: ["Gaussian Error Linear Units (GELUs)"](https://arxiv.org/abs/1606.08415)

---

### GELU的数学表达式

GELU函数的数学表达式为：

```
GELU(x) = x * P(X ≤ x) = x * Φ(x)
```

其中：

* `x` 是输入值。
* `P(X ≤ x)` 表示一个标准正态分布的累积分布函数（CDF），通常用 `Φ(x)` 表示。

<br>

### GELU的直观解释

* **非线性变换**：GELU函数通过将输入乘以一个概率值来实现非线性变换。
* **平滑过渡**：GELU函数的曲线比ReLU更加平滑，这有助于缓解梯度消失问题，提高模型的训练效率。
* **自适应性**：GELU函数的输出取决于输入的大小，对于较大的输入，输出接近于输入本身；对于较小的输入，输出接近于0。这种自适应性使得GELU函数能够更好地适应不同的数据分布。

<br>

### GELU的优点

* **性能优异**：在许多任务上，GELU都表现出了比ReLU更好的性能。
* **理论基础扎实**：GELU函数的数学基础比较严谨，具有较好的可解释性。
* **平滑过渡**：缓解了梯度消失问题，提高了模型的训练效率。

<br>

### GELU的实现

由于标准正态分布的累积分布函数没有解析解，因此在实际应用中，通常使用以下近似公式来计算GELU：

```
GELU(x) ≈ 0.5 * x * (1 + tanh(sqrt(2/π) * (x + 0.044715x^3)))
```

<br>

### 公式
GELU的数学表达式为：

\[ \text{GELU}(x) = x \cdot \Phi(x) \]

其中，\(\Phi(x)\) 是标准正态分布的累积分布函数（CDF），即：

\[ \Phi(x) = \frac{1}{2} \left( 1 + \text{erf} \left( \frac{x}{\sqrt{2}} \right) \right) \]

这里，\(\text{erf}\) 是误差函数。

### 近似计算
由于GELU的计算涉及误差函数，实际应用中常使用近似公式以提高计算效率。常用的近似公式为：

\[ \text{GELU}(x) \approx 0.5x \left( 1 + \tanh\left( \sqrt{\frac{2}{\pi}} \left( x + 0.044715x^3 \right) \right) \right) \]



<br>


### GELU与ReLU的对比

| 特征 | ReLU | GELU |
|---|---|---|
| 数学表达式 | max(0, x) | x * Φ(x) |
| 曲线形状 | 分段线性 | 平滑曲线 |
| 优点 | 计算简单，收敛速度快 | 性能更好，理论基础扎实 |
| 缺点 | 可能导致神经元死亡 | 计算复杂度略高 |

<br>


### 总结

GELU函数作为一种新型的激活函数，在深度学习领域受到了广泛关注。它的性能优异、理论基础扎实，使得它成为了许多神经网络模型的首选激活函数。


<br>

**GELU函数的优势主要体现在以下几个方面：**

* **更好的非线性**：GELU函数的非线性变换能力比ReLU更强，能够更好地拟合复杂的数据分布。
* **更平滑的过渡**：GELU函数的曲线更加平滑，有助于缓解梯度消失问题，提高模型的训练效率。
* **更好的泛化性能**：GELU函数在许多任务上都表现出了更好的泛化性能。

