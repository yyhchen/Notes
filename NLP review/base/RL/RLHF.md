# RLHF

---

[知乎 RLHF](https://zhuanlan.zhihu.com/p/657490625)


<br>
<br>

### ChatGPT需要有大致三个阶段的训练过程

- **Pretraining:** 在大规模“无监督”的语料上训练，训练任务是预测下一个词。
- **Supervised Fine-Tuning（SFT）:** 在人类标注上进行微调，所谓人类标注就是人类写Prompt，人类写答案。然后语言模型学习模仿人类是如何作答的。这部分通常要求数据集多样性很好，也因为标注成本很高，通常量级很小。
- **Reinforcement Learning with human feedback（RLHF）:** 对于同一个Prompt把模型的多个输出给人类排序，获取人类偏好标注。用人类的偏好标注，训练一个reward model。训练得到的reward model会作为PPO算法中的reawrd function，来继续优化SFT得到的模型。






### RLHF中的PPO算法

在RLHF（基于人类反馈的强化学习）中，目标函数设计为**奖励模型的期望减去RL策略与SFT策略的KL散度**，主要目的是**平衡奖励最大化与策略稳定性**。以下是关键原因：

---

#### 1. **防止奖励过优化（Reward Hacking）**
   - **问题**：若直接最大化奖励模型的输出（E[r]），策略可能通过“欺骗”奖励模型获得高回报，但生成的内容可能不符合人类真实偏好（例如生成无意义但符合奖励模型指标的高分文本）。
   - **解决方法**：KL散度作为正则化项，约束RL策略与SFT策略（监督微调后的初始策略）的偏离程度，确保策略更新不会过于激进。

---

#### 2. **维持生成内容的质量与安全性**
   - **SFT策略的作用**：SFT策略是基于高质量人类标注数据训练的初始策略，生成内容相对可靠。
   - **KL散度的意义**：通过最小化RL策略与SFT策略的差异（KL散度），确保强化学习阶段不会破坏SFT阶段学到的合理行为，避免生成有害或偏离预期的内容。

---

#### 3. **数学视角：约束优化问题**
   RLHF的目标可形式化为带约束的优化问题：

  $$ \text{最大化} \quad \mathbb{E}_{x \sim \pi_{\text{RL}}} [r(x)] \quad \text{且满足} \quad \text{KL}(\pi_{\text{RL}} \| \pi_{\text{SFT}}) \leq \delta$$

   - **拉格朗日松弛**：通过引入KL散度惩罚项，将约束优化转化为无约束优化问题：
     
     $$\text{目标函数} = \mathbb{E}[r(x)] - \beta \cdot \text{KL}(\pi_{\text{RL}} \| \pi_{\text{SFT}})$$
     
     其中，超参数 $\beta$ 控制奖励与保守性之间的权衡。

---

#### 4. **信息论视角：最小化策略偏移**
   - KL散度衡量策略分布差异，最小化KL散度等价于**限制策略更新的信息量**（参考**信任区域优化**）。
   - 避免策略在单次更新中大幅偏离当前策略，从而保证训练稳定性。

---

#### 5. **对抗奖励模型的不完美性**
   - **奖励模型的局限性**：奖励模型可能对某些输入敏感或存在偏差，过度优化会导致策略利用这些漏洞。
   - **KL惩罚的鲁棒性**：通过约束策略接近SFT策略（人类专家数据分布），降低对奖励模型缺陷的敏感性。

---

#### 总结公式的意义

$$\text{目标函数} = \underbrace{\mathbb{E}[r(x)]}_{\text{最大化奖励}} - \underbrace{\beta \cdot \text{KL}(\pi_{\text{RL}} \| \pi_{\text{SFT}})}_{\text{约束策略偏移}}$$

- **奖励项**：驱动策略生成高回报内容。
- **KL项**：作为“锚点”防止策略偏离合理行为，避免过度优化。

这种设计是RLHF在**对齐（Alignment）** 与**安全性（Safety）** 之间权衡的核心机制。