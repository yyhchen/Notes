# flash attention

---



### 核心思想

- tiling
- recomputation


<br>
<br>


#### recomputation 

在深度学习模型中，特别是在处理长序列时，内存和计算资源是关键的限制因素。为了解决这个问题，研究者们提出了多种方法，其中包括recomputation（重计算）技术。Recomputation是一种在训练过程中减少内存使用的技术，它通过在反向传播过程中重新计算某些中间激活值来避免存储这些值。

**Recomputation的基本思想**：
在标准的深度学习模型训练过程中，前向传播阶段计算的所有中间激活值都需要被存储下来，以便在反向传播阶段使用。这是因为反向传播需要这些中间值来计算梯度。然而，存储所有这些中间值会占用大量内存，尤其是在处理非常深的网络或长序列时。
Recomputation技术允许我们**在前向传播时只存储一部分中间激活值，而在反向传播需要时重新计算那些被丢弃的激活值**。这样，我们可以在不牺牲计算精度的情况下显著减少内存使用。

**Recomputation的实现方式**：
1. **检查点（Checkpointing）**：这是一种常见的recomputation方法，它在前向传播过程中的特定点设置检查点，并只存储这些检查点处的激活值。在反向传播时，如果需要之前丢弃的激活值，就从最近的检查点开始重新计算。
2. **按需计算（On-Demand Computation）**：在反向传播过程中，只有当某个中间激活值的梯度被需要时，才重新计算该激活值。这种方法可以进一步减少不必要的计算。
3. **内存交换（Memory Swap）**：这是一种将中间激活值存储到辅助存储（如硬盘或固态硬盘）中的技术，当需要时再加载回内存。虽然这种方法可以减少内存使用，但由于I/O操作的速度较慢，它可能会增加计算时间。

**Recomputation的挑战**：
- **计算开销**：虽然recomputation可以减少内存使用，但它可能会增加计算时间，因为需要重新计算某些中间激活值。
- **实现复杂性**：正确实现recomputation需要对模型结构和训练流程有深入的理解，以确保在需要时能够正确地重新计算中间激活值。
- **梯度精确性**：在某些情况下，由于数值精度的原因，recomputation可能会导致梯度的轻微变化，这可能会影响模型的训练效果。
总的来说，recomputation是一种在内存受限的情况下提高深度学习模型训练效率的有效技术。通过精心设计检查点和计算策略，可以在不显著影响模型性能的情况下减少内存使用。
