# flash attention

---



### 核心思想

- tiling
- recomputation


<br>
<br>


#### recomputation 

在深度学习模型中，特别是在处理长序列时，内存和计算资源是关键的限制因素。为了解决这个问题，研究者们提出了多种方法，其中包括recomputation（重计算）技术。Recomputation是一种在训练过程中减少内存使用的技术，它通过在反向传播过程中重新计算某些中间激活值来避免存储这些值。

**Recomputation的基本思想**：
在标准的深度学习模型训练过程中，前向传播阶段计算的所有中间激活值都需要被存储下来，以便在反向传播阶段使用。这是因为反向传播需要这些中间值来计算梯度。然而，存储所有这些中间值会占用大量内存，尤其是在处理非常深的网络或长序列时。
Recomputation技术允许我们**在前向传播时只存储一部分中间激活值，而在反向传播需要时重新计算那些被丢弃的激活值**。这样，我们可以在不牺牲计算精度的情况下显著减少内存使用。

**Recomputation的实现方式**：
1. **检查点（Checkpointing）**：这是一种常见的recomputation方法，它在前向传播过程中的特定点设置检查点，并只存储这些检查点处的激活值。在反向传播时，如果需要之前丢弃的激活值，就从最近的检查点开始重新计算。
2. **按需计算（On-Demand Computation）**：在反向传播过程中，只有当某个中间激活值的梯度被需要时，才重新计算该激活值。这种方法可以进一步减少不必要的计算。
3. **内存交换（Memory Swap）**：这是一种将中间激活值存储到辅助存储（如硬盘或固态硬盘）中的技术，当需要时再加载回内存。虽然这种方法可以减少内存使用，但由于I/O操作的速度较慢，它可能会增加计算时间。

**Recomputation的挑战**：
- **计算开销**：虽然recomputation可以减少内存使用，但它可能会增加计算时间，因为需要重新计算某些中间激活值。
- **实现复杂性**：正确实现recomputation需要对模型结构和训练流程有深入的理解，以确保在需要时能够正确地重新计算中间激活值。
- **梯度精确性**：在某些情况下，由于数值精度的原因，recomputation可能会导致梯度的轻微变化，这可能会影响模型的训练效果。
总的来说，recomputation是一种在内存受限的情况下提高深度学习模型训练效率的有效技术。通过精心设计检查点和计算策略，可以在不显著影响模型性能的情况下减少内存使用。


---

Flash Attention 的两个核心技术是 **Tiling**（分块）和 **Recomputation**（重计算）。这两种技术通过优化内存访问和计算顺序，显著提高了注意力机制的计算效率，特别是在处理长序列时。

### 1. **Tiling（分块）**
Tiling 是一种将大矩阵分成多个小块（Blocks）的技术，以便在每个小块上进行局部计算。通过分块，可以减少内存访问次数，提高计算效率。

#### 1.1 **Tiling 的基本思想**
在标准的注意力机制中，计算 Query、Key 和 Value 之间的矩阵乘法时，需要一次性加载整个矩阵到内存中。这对于长序列来说是非常耗费内存的。Tiling 通过将输入矩阵分成多个小块，并在每个小块上进行局部计算，减少了内存访问次数。

#### 1.2 **Tiling 的实现**
Tiling 的实现通常包括以下几个步骤：
1. **分块**：将输入矩阵（如 Query、Key、Value）分成多个小块。每个小块的大小可以根据内存带宽和计算能力进行调整。
2. **局部计算**：在每个小块上进行局部计算，包括矩阵乘法和 Softmax 操作。
3. **结果合并**：将每个小块的计算结果合并，得到最终的注意力矩阵。

#### 1.3 **Tiling 的优势**
- **减少内存访问**：通过分块计算，减少了内存访问次数，提高了计算效率。
- **适用于长序列**：特别适用于处理长序列，因为它可以有效地减少内存访问和计算开销。
- **并行计算**：分块计算可以充分利用硬件的并行计算能力，提高计算速度。

### 2. **Recomputation（重计算）**
Recomputation 是一种通过在反向传播过程中重新计算前向传播的中间结果，来减少内存使用的技术。在标准的注意力机制中，前向传播的中间结果（如 Softmax 的输入和输出）需要存储在内存中，以便在反向传播时使用。Recomputation 通过在反向传播时重新计算这些中间结果，减少了内存使用。

#### 2.1 **Recomputation 的基本思想**
在标准的注意力机制中，前向传播的中间结果需要存储在内存中，以便在反向传播时使用。这对于长序列来说是非常耗费内存的。Recomputation 通过在反向传播时重新计算这些中间结果，减少了内存使用。

#### 2.2 **Recomputation 的实现**
Recomputation 的实现通常包括以下几个步骤：
1. **前向传播**：在前向传播过程中，只存储必要的中间结果（如 Softmax 的输入和输出）。
2. **反向传播**：在反向传播过程中，重新计算前向传播的中间结果，而不是从内存中读取。
3. **梯度计算**：使用重新计算的中间结果进行梯度计算。

#### 2.3 **Recomputation 的优势**
- **减少内存使用**：通过在反向传播时重新计算中间结果，减少了内存使用，特别适用于处理长序列。
- **提高计算效率**：虽然重新计算会增加计算量，但由于减少了内存访问，整体计算效率仍然可以提高。
- **适用于长序列**：特别适用于处理长序列，因为它可以有效地减少内存使用和计算开销。

### 3. **Tiling 和 Recomputation 的结合**
在实际应用中，Tiling 和 Recomputation 通常结合使用，以进一步提高计算效率和减少内存使用。具体来说：
- **Tiling**：通过分块计算，减少内存访问次数，提高计算效率。
- **Recomputation**：通过在反向传播时重新计算中间结果，减少内存使用，特别适用于处理长序列。

### 4. **总结**
Flash Attention 的两个核心技术 Tiling 和 Recomputation 通过优化内存访问和计算顺序，显著提高了注意力机制的计算效率。Tiling 通过分块计算减少了内存访问次数，提高了计算效率；Recomputation 通过在反向传播时重新计算中间结果，减少了内存使用，特别适用于处理长序列。这两种技术结合使用，可以进一步提高计算效率和减少内存使用。


<br>

<br>

<br>

# 疑问


### “Tiling 通过将输入矩阵分成多个小块，并在每个小块上进行局部计算，减少了内存访问次数。”怎么理解这句话？分成多个小块进行计算，不是增加了内存访问次数吗？

你提到的观点是正确的，从表面上看，一次性加载整个矩阵只需要访问一次内存，而分块计算需要多次访问内存。然而，实际情况并非如此简单，因为内存访问的效率和计算的复杂度需要综合考虑。

### 1. **内存带宽和延迟**
在现代计算系统中，内存带宽和延迟是两个关键因素。内存带宽是指内存系统在单位时间内可以传输的数据量，而内存延迟是指从发出内存请求到数据可用的时间。

#### 1.1 **内存带宽**
- **一次性加载整个矩阵**：虽然只需要一次内存访问，但如果矩阵非常大，可能会超过内存带宽的限制，导致内存访问成为瓶颈。
- **分块计算**：通过将矩阵分成多个小块，可以减少每次内存访问的数据量，从而更好地利用内存带宽。

#### 1.2 **内存延迟**
- **一次性加载整个矩阵**：如果矩阵非常大，一次性加载可能会导致内存延迟增加，因为需要等待所有数据加载完成。
- **分块计算**：通过分块计算，可以减少内存延迟，因为每次只需要加载和处理一个小块的数据。

### 2. **局部性和缓存利用**
局部性（Locality）是指程序在执行过程中倾向于访问相邻的内存位置。局部性分为时间局部性和空间局部性。

#### 2.1 **时间局部性**
- **一次性加载整个矩阵**：如果矩阵非常大，数据可能不会在缓存中停留很长时间，导致缓存命中率低。
- **分块计算**：通过分块计算，可以提高时间局部性，因为每次只需要加载和处理一个小块的数据，数据在缓存中停留的时间更长。

#### 2.2 **空间局部性**
- **一次性加载整个矩阵**：如果矩阵非常大，数据可能不会在缓存中连续存储，导致空间局部性差。
- **分块计算**：通过分块计算，可以提高空间局部性，因为每次只需要加载和处理一个小块的数据，数据在缓存中连续存储。

### 3. **并行计算**
分块计算还可以充分利用硬件的并行计算能力，提高计算速度。通过将矩阵分成多个小块，可以在多个处理单元上并行计算，从而减少整体的计算时间。

### 4. **实际例子**
假设我们有一个大矩阵 \( Q \)（Query）、\( K \)（Key）和 \( V \)（Value），大小为 \( N \times D \)，其中 \( N \) 是序列长度，\( D \) 是特征维度。在标准的注意力机制中，计算注意力权重时，需要一次性加载整个矩阵 \( Q \) 和 \( K \) 到内存中。

在 Tiling 技术中，我们将矩阵 \( Q \) 和 \( K \) 分成多个小块，例如每个小块的大小为 \( B \times D \)，其中 \( B \) 是块大小。在每个小块上进行局部计算，只需要加载当前小块的数据，而不需要一次性加载整个矩阵。这样可以减少内存访问的次数，提高计算效率。

### 5. **总结**
虽然从表面上看，一次性加载整个矩阵只需要访问一次内存，而分块计算需要多次访问内存，但实际上，分块计算通过减少每次内存访问的数据量、提高局部性和缓存利用率、充分利用并行计算能力，减少了整体的内存访问开销，提高了计算效率。因此，Tiling 技术通过优化内存访问模式，减少了内存带宽的瓶颈，提高了计算效率。