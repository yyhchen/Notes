# 协方差与方差 （知识补充）

---


### 什么是协方差
协方差是衡量两个随机变量如何一起变化的统计量。它是统计学中用于描述两个变量之间线性关系程度的一个概念。协方差的值可以是正的、负的或者零，不同的值表示不同的关系：
1. **正协方差（Positive Covariance）：** 如果两个变量的值同时倾向于高于它们的平均值，那么它们之间就存在正协方差。这意味着当一个变量的值增加时，另一个变量的值也倾向于增加。
2. **负协方差（Negative Covariance）：** 如果两个变量的值同时倾向于低于它们的平均值，那么它们之间就存在负协方差。这意味着当一个变量的值增加时，另一个变量的值倾向于减少。
3. **零协方差（Zero Covariance）：** 如果两个变量之间没有线性关系，即一个变量的值的变化不会导致另一个变量的值有可预测的变化，那么它们之间的协方差为零。但这并不意味着两个变量完全独立，它们之间可能存在非线性关系。

**协方差的计算公式为：**

\[ \text{Cov}(X, Y) = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{n-1} \]

其中，\( x_i \) 和 \( y_i \) 是两个变量的第 \( i \) 个观测值，\( \bar{x} \) 和 \( \bar{y} \) 是它们的样本平均值，\( n \) 是观测值的数量。

协方差的值受到变量单位的影响，因此它本身并**不直接提供**一个关于变量关系强度的度量。为了解决这个问题，通常会计算相关系数（Pearson 相关系数），它是协方差除以两个变量标准差的乘积，这样可以得到一个无单位的度量，其值介于 -1 和 1 之间，可以更好地描述变量之间的线性关系程度。


<br>
<br>


### 相关系数 (能否更准确描述两个变量之间的关系呢？)
相关系数（Pearson 相关系数）确实能够提供一种标准化了的度量，用来描述两个变量之间线性关系的强度和方向。**相比于协方差，==相关系数的优势在于它是一个无单位的度量==，并且其值被限制在 -1 和 1 之间，这使得相关系数更容易解释和理解**。

以下是相关系数的几个重要特点：
1. **值域**：相关系数的值域在 -1 到 1 之间，其中：
   - 1 表示完全正相关，即一个变量的增加必然伴随着另一个变量的增加，且它们之间是直线关系。
   - -1 表示完全负相关，即一个变量的增加必然伴随着另一个变量的减少，且它们之间是直线关系。
   - 0 表示没有线性相关，即两个变量之间没有直线关系，但可能存在其他类型的关系（如非线性关系）。
2. **标准化**：相关系数是通过将协方差除以两个变量标准差的乘积来计算的，因此它是一个无量纲的度量，不会受到变量单位的影响。
3. **强度**：相关系数的绝对值越接近 1，表示两个变量之间的线性关系越强；绝对值越接近 0，表示线性关系越弱。
4. **方向**：相关系数的正负号表示变量之间关系的方向，正值表示正相关，负值表示负相关。
尽管相关系数在描述两个变量之间的线性关系时非常有用，但它也有一些局限性：
- **线性关系**：相关系数只能够检测和量化两个变量之间的线性关系，对于非线性关系，即使两个变量之间存在很强的关系，相关系数也可能接近于 0。
- **独立性**：相关系数为 0 并不意味着两个变量完全独立，它们之间可能存在非线性关系或者在其他方面相关。
- **异常值敏感**：相关系数对于异常值比较敏感，少量的异常值可能会显著影响相关系数的值。
- 
因此，虽然相关系数是一个非常有用的统计工具，但它并不是描述两个变量之间关系的唯一方法，也不是万能的。**在实际应用中，需要结合具体问题和数据的特点，以及其他统计和可视化方法，来全面理解两个变量之间的关系**。


<br>
<br>


### 协方差计算为什么分母是n-1？协方差跟方差有什么关系？什么时候协方差能变成方差？

协方差的计算中使用分母 \( n-1 \) 而不是 \( n \) 是因为在统计学中，我们通常使用样本而不是整个总体来进行推断。==**当我们从总体中抽取一个样本时，这个样本的统计量（如样本均值、样本方差）往往会略低于总体参数的真实值**==。为了校正这种偏差，我们使用 \( n-1 \) 作为分母，这被称为贝塞尔校正（Bessel's correction）。**贝塞尔校正使得==样本方差==成为==总体方差==的无偏估计量**。

协方差与方差之间的关系在于，协方差是衡量两个变量共同变化的程度，而方差是衡量一个变量自身变化的程度。实际上，协方差可以看作是两个变量之间关系的扩展，而方差可以看作是一个变量与自身的协方差。

**具体来说，协方差的计算公式是：**
\[ \text{Cov}(X, Y) = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{n-1} \]

其中，\( x_i \) 和 \( y_i \) 是两个变量的第 \( i \) 个观测值，\( \bar{x} \) 和 \( \bar{y} \) 是它们的样本平均值，\( n \) 是观测值的数量。

**方差的计算公式是：**
\[ \text{Var}(X) = \frac{\sum_{i=1}^{n} (x_i - \bar{x})^2}{n-1} \]

其中，\( x_i \) 是变量的第 \( i \) 个观测值，\( \bar{x} \) 是变量的样本平均值，\( n \) 是观测值的数量。

可以看出，**方差是协方差的一个特例，即当两个变量是同一个变量时的情况**。如果我们计算一个变量与自身的协方差，那么我们实际上就是在计算这个变量的方差。这是因为 \( (x_i - \bar{x})(x_i - \bar{x}) \) 等于 \( (x_i - \bar{x})^2 \)，这正是方差的计算公式中的部分。

协方差能变成方差的情况就是当一个变量在协方差公式中重复出现时。例如，如果我们计算 \( \text{Cov}(X, X) \)，那么结果就是 \( \text{Var}(X) \)。这是因为协方差公式中的两个变量 \( X \) 和 \( X \) 是相同的，所以协方差公式中的 \( (x_i - \bar{x})(x_i - \bar{x}) \) 部分就变成了方差的计算公式。



<br>
<br>



### 计算协方差，什么时候用 n-1 或者 n 作为分母
协方差是衡量两个随机变量如何一起变化的度量。在计算样本协方差时，使用n-1作为分母的情况出现在当你有样本数据，并且你想要估计总体协方差时。这种做法是基于样本方差的无偏估计，也就是所谓的Bessel's correction。
当你有完整的数据集，即总体数据时，你应该使用n作为分母来计算协方差，因为这时你不需要估计，而是直接计算总体的协方差。

具体来说：
1. **当你有样本数据**，并且想要从这些样本来估计总体协方差的期望值时，使用n-1作为分母。这是因为在样本方差中，使用n-1会给出一个无偏的估计量，即样本方差是总体方差的无偏估计。同样地，样本协方差也是这样处理的，以保持估计的无偏性。
   样本协方差的无偏估计公式为：
   \[ s_{xy} = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y}) \]
   其中，\( s_{xy} \)是样本协方差，\( x_i \)和\( y_i \)是样本数据点，\( \bar{x} \)和\( \bar{y} \)分别是x和y的样本均值，n是样本大小。
2. **当你有总体数据**，或者你不在意估计的无偏性，而是直接计算样本内的协方差时，使用n作为分母。
   总体协方差的计算公式为：
   \[ \sigma_{xy} = \frac{1}{n} \sum_{i=1}^{n} (x_i - \mu_x)(y_i - \mu_y) \]
   其中，\( \sigma_{xy} \)是总体协方差，\( \mu_x \)和\( \mu_y \)分别是x和y的总体均值。

总结来说，使用n-1作为分母是为了得到一个无偏的估计量，这在统计学中是非常重要的，因为它可以防止由于样本大小而导致的估计偏差。当你有总体数据时，使用n作为分母，因为这时你不需要估计，可以直接计算真实的协方差。


<br>
<br>
<br>


### 如果 n-1 是无偏估计量，n-2 行不行呢

不行， `n-1` 是数学推理证明的过程

---

使用n-1作为分母来计算样本方差（以及样本协方差）是基于这样一个事实：对于从一个正态分布的总体中抽取的样本，样本方差是总体方差的无偏估计。这是由统计学家卡尔·皮尔逊（Karl Pearson）在19世纪末提出的，并且在统计学中被称为Bessel's correction。
**选择n-1而不是n-2或其他值的原因是基于数学上的推导和证明**。当从正态分布的总体中抽取样本时，**样本方差的期望值（即平均值）会略小于总体方差**。为了校正这个偏差，我们需要从样本方差中减去一个修正项。这个修正项恰好是样本均值与总体均值之差的平方的平均值，而这个平均值在样本大小为n时等于1/n。因此，为了得到无偏估计，我们需要将样本方差乘以n/n-1。

**数学上，可以这样推导：**
设 $X1, X2, ..., Xn$ 是来自正态分布 $N(μ, σ^2)$ 的独立同分布样本，样本方差为 $s^2$，总体方差为 $σ^2$。 那么，样本方差的期望值 $E[s^2]$ 可以表示为：
\[ E[s^2] = E\left[\frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})^2\right] \]

由于样本均值的期望值等于总体均值（$E[bar{X}] = μ$），我们可以将样本方差展开为：
\[ E[s^2] = \frac{1}{n-1} \left[ (n-1)σ^2 + σ^2 \right] = \frac{nσ^2}{n-1} \]

为了使 $E[s^2]$ 等于 $σ^2$，我们需要将 $s^2$ 乘以 $(n-1)/n$，这样就得到了无偏估计量的公式：
\[ s^2 = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})^2 \]

同样地，这个修正也适用于样本协方差的计算，因为协方差在计算上与方差相似，只是涉及两个不同的变量。
如果选择n-2或其他值作为分母，那么得到的估计量将不再是无偏的，因为这样的修正不符合从样本数据中估计总体参数的数学原理。因此，n-1是正确的修正值，以确保估计量的无偏性。
