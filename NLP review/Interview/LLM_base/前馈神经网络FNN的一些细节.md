
---

FNN（前馈神经网络，Feedforward Neural Network）的公式取决于其层数设计，具体分为以下两种情况：


### **1. 单层无隐藏层的FNN（即逻辑回归/单层感知机）**
- **公式**：`y = x·w + b`  
  - 输入 `x` 直接通过权重 `w` 和偏置 `b` 映射到输出 `y`。
  - **本质**：这是最简单的线性模型，无隐藏层，仅输入层和输出层。
  - **应用场景**：二分类（如逻辑回归）、线性回归。

> Linear 层，Dense层也是这个单层感知机
---

### **2. 含隐藏层的FNN（多层感知机，MLP）**
- **公式**：`y = σ(x·w1 + b1)·w2 + b2`  
  - **输入层 → 隐藏层**：`h = σ(x·w1 + b1)`（加入非线性激活函数，如ReLU、Sigmoid）。
  - **隐藏层 → 输出层**：`y = h·w2 + b2`。
  - **本质**：==含至少一个隐藏层，激活函数引入非线性表达能力==。
  - **注意**：如果省略激活函数（即 `σ` 是恒等函数），则退化为线性叠加，等效于单层网络。

---

### **关键区别**
| **模型类型**       | 公式                          | 是否含隐藏层 | 非线性能力 |
|--------------------|-------------------------------|--------------|------------|
| 单层FNN（感知机）  | `y = x·w + b`                 | 无           | 无         |
| 多层FNN（MLP）     | `y = σ(x·w1 + b1)·w2 + b2`    | 有           | 有         |

---

### **常见误解澄清**
1. **FNN ≠ 单层网络**  
   FNN泛指前馈结构，包含多层时称为深度前馈网络（Deep FNN）。  
   **“多层”是FNN的核心特性**，单层仅为特例。

2. **激活函数的作用**  
   即使公式写作 `(x·w1 + b1)·w2 + b2`，若省略激活函数（如ReLU），则实际退化为：  
   `y = x·(w1·w2) + (b1·w2 + b2)`，仍然是线性模型，失去多层意义。

3. **通用近似定理**  
   含一个隐藏层+非线性激活的FNN（MLP）可以逼近任意连续函数，这是深度学习的理论基础。

---

### **总结**
- **若问题中的FNN指单层网络**，公式是 `x·w + b`。  
- **若FNN包含隐藏层**，公式应为 `σ(x·w1 + b1)·w2 + b2`（必须包含激活函数）。  
- **实际代码中**，多层FNN的实现通常逐层定义：  
  ```python
  h = torch.relu(x @ w1 + b1)  # 隐藏层（含激活函数）
  y = h @ w2 + b2              # 输出层
  ```