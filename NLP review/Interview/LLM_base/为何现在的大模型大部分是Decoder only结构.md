
---

>[!NOTE]
>1. 因为decoder-only结构模型**在没有任何微调数据的情况下，zero-shot的表现能力最好**。而encoder-decoder则需要在一定量的标注数据上做 multitask-finetuning 才能够激发最佳性能。
>
>2. 大模型使用decoder-only架构除了训练效率和工程实现上的优势外，**在理论上因为Encoder的双向注意力会存在低秩的问题，这可能会削弱模型的表达能力**。



那么问题来了，低秩问题是什么？

>[!tip] 
>双向注意力可能会导致 **注意力权重矩阵的行或列之间** 存在强相关性！ 这样矩阵的秩就会远低于最大值(n)。



## 详细解释


在Transformer模型中，**"encoder的双向注意力存在低秩问题"** 这句话指的是：在编码器（encoder）的自注意力机制（self-attention）中，注意力矩阵（attention matrix）的秩（rank）可能较低，导致其表达能力受限。这种现象可能影响模型捕捉复杂依赖关系的能力。以下是详细解释：

---

### **1. 什么是"低秩问题"？**
- **矩阵的秩（Rank）**：矩阵中线性无关的行或列的数量。秩越低，矩阵的信息冗余度越高，表达能力越弱。
- **注意力矩阵的低秩**：在自注意力中，计算出的注意力权重矩阵（例如，序列长度为n时是n×n矩阵）可能只有少数几个主成分（dominant singular values）占主导地位，导致其秩远低于理论最大值（n）。

---

### **2. 双向注意力中的低秩现象**
在Transformer的encoder中，自注意力是**双向的**（每个token可以关注序列中任意位置的token）。低秩问题的具体表现：
- **冗余的注意力模式**：==不同位置的token可能关注相似的上文/下文，导致注意力权重矩阵的行或列之间存在强相关性==。
- **长距离依赖的稀疏性**：长序列中，大部分token之间的注意力权重可能趋近于0，矩阵中有效信息集中在少数位置（例如对角线附近），进一步降低秩。
- **数学验证**：对注意力矩阵进行奇异值分解（SVD）时，会发现大部分奇异值接近于0，仅有少数较大的奇异值（如下图示意）。

---

### **3. 低秩问题的原因**
1. **数据本身的冗余性**  
   输入序列中可能存在重复或高度相关的信息（例如自然语言中的高频词、语法结构等），导致注意力权重矩阵的冗余。

2. **Softmax的归一化特性**  
   Softmax函数会将注意力权重压缩到少数几个显著的位置（"赢者通吃"效应），加剧矩阵的稀疏性和低秩性。

3. **位置编码的局限性**  
   绝对位置编码或相对位置编码可能无法充分建模复杂的全局依赖关系，导致注意力模式趋于简单化。

---

### **4. 低秩问题的影响**
- **模型表达能力下降**：低秩矩阵难以捕捉细粒度的、多样化的依赖关系（例如长距离的复杂交互）。
- **信息瓶颈**：注意力矩阵的有效信息集中在少数维度，可能导致模型忽略局部细节或罕见模式。
- **计算效率的假象**：虽然低秩矩阵可通过近似加速计算（如使用低秩分解），但这是以牺牲性能为代价的。

---

### **5. 解决方案与改进方向**
1. **显式引入高秩机制**  
   - 使用多头注意力（Multi-Head Attention）：通过多个独立的注意力头（每个头关注不同模式）提升整体秩。
   - 增加注意力头的多样性（如混合不同尺度的注意力）。

2. **改进注意力计算方式**  
   - **稀疏注意力**（Sparse Attention）：强制关注局部或关键区域，避免全局注意力矩阵的低秩化（如Longformer、BigBird）。
   - **低秩近似+残差补偿**：将注意力矩阵分解为低秩部分（快速计算）和高秩残差部分（捕捉细节），如Linformer。

3. **增强位置编码**  
   - 使用更复杂的位置编码（如旋转位置编码RoPE），提升模型对位置关系的建模能力。

4. **引入先验知识**  
   - 通过结构化注意力（如语法树引导的注意力）或外部记忆模块补充低秩矩阵的信息缺失。

---

### **6. 总结**
"双向注意力存在低秩问题"本质上是Transformer模型在处理长序列或复杂依赖关系时的一种局限性。理解这一问题有助于设计更高效的注意力机制（如稀疏化、分块处理），或结合外部知识增强模型能力（如检索增强的Transformer）。当前的研究（如Perceiver、FlashAttention）也在尝试从根本上突破这一瓶颈。