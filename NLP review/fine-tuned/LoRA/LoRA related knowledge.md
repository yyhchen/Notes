# LoRA 涉及到的一些数学知识等

用LoRA fine-tuning 能够用少量参数媲美全参数的效果。（**非常重要的一个fine-tuned技术**）

---


### LoRA核心，rank decomposition matrices

==（这是LoRA为什么可以计算量小的重要原因）==

矩阵的秩分解（Rank Decomposition of Matrices），也称为矩阵的秩因子分解，**是指将一个矩阵分解为几个低秩矩阵的乘积的过程**。这种分解在数据压缩、降维和优化问题等方面非常有用。**最常见的秩分解形式是低秩近似，即通过几个秩很小的矩阵的乘积来近似原始矩阵，这有助于减少计算复杂度和存储需求**。
秩分解的方法有多种，包括：
1. **奇异值分解**（Singular Value Decomposition, SVD）：这是最常用的秩分解方法。通过SVD，一个\(m \times n\)的矩阵\(A\)可以被分解为三个矩阵\(U\)、\(S\)和\(V\)的乘积，其中\(U\)和\(V\)是正交矩阵，\(S\)是对角矩阵。分解形式如下：
   \[ A = U \cdot S \cdot V^T \]
   通过保留\(S\)中对角线上最大的\(k\)个奇异值（\(k\)是期望的秩），我们可以得到\(A\)的一个低秩近似。
2. **主成分分析**（Principal Component Analysis, PCA）：PCA是一种统计技术，也可以用于秩分解。当矩阵\(A\)表示一个高维特征的数据集时，PCA特别有用。PCA寻找数据变化最大的方向（主成分），并将数据投影到这些方向上，从而得到一个低维表示。
3. **非负矩阵分解**（Non-negative Matrix Factorization, NMF）：NMF是一种将非负矩阵分解为两个非负矩阵乘积的方法。当处理自然是非负的数据时，如图像数据或文本数据，这种分解非常有用。
4. **张量分解**：对于作为高阶张量一部分的矩阵，可以使用张量分解方法，如典范多项式分解（Canonical Polyadic Decomposition, CP）或Tucker分解，将张量分解为若干个秩-1张量的和，这实际上是将矩阵分解为若干个秩-1矩阵的和。
这些方法具有不同的性质，适用于不同类型的数据和应用。选择哪种方法取决于具体问题的需求，如是否需要非负性、可解释性或计算效率。


<br>
<br>
<br>

### 秩分解和特征分解的区别
==（也就是说，rank decomposition 跟 常见的特征分解不是一个东西！线代基础）==

**将矩阵分解为特征值和特征矩阵通常被称为特征分解**（Eigenvalue Decomposition），**而不是秩分解**（Rank Decomposition）。特征分解是一种特定的矩阵分解方法，它将一个方阵（即行数和列数相等的矩阵）分解为特征值和特征向量的乘积。
对于一个方阵 \( A \)，如果它有 \( n \) 个线性无关的特征向量 \( v_1, v_2, ..., v_n \) 和对应的特征值 \( \lambda_1, \lambda_2, ..., \lambda_n \)，那么特征分解可以表示为：
\[ A = V \Lambda V^{-1} \]
其中，\( V \) 是由特征向量构成的矩阵，\( \Lambda \) 是对角矩阵，其对角线上的元素是特征值 \( \lambda_1, \lambda_2, ..., \lambda_n \)。

**特征分解的主要目的是为了简化矩阵的运算**，例如计算矩阵的幂、求解线性微分方程等。特征分解揭示了矩阵的内在结构，比如矩阵的秩等于其非零特征值的个数。

虽然特征分解和秩分解都是矩阵分解的方法，但它们的目的和应用场景不同。**秩分解**通常用于降低矩阵的维度或近似矩阵，而**特征分解**用于分析矩阵的性质和结构。在秩分解中，我们通常关注的是如何用低秩的矩阵来近似原始矩阵，而在特征分解中，我们关注的是矩阵的特征值和特征向量。


<br>
<br>
<br>



### 又涉及到主成分分析（PCA）和奇异值分解（SVD）

目的就是降维，因为 LoRA 就是低秩分解的过程。


<br>
<br>

---





## LoRA 的一些结论

- 初始化矩阵B和矩阵A时，**B初始化为0**，A进行高斯分布的随机初始化。
- 
- 选择低秩矩阵时，**选择多个待调矩阵**并提供 **较小r** 的策略**比选择单一矩阵**并提供 **较大r** 的策略好。
- 
- 首先在r的选择上，取r=1已经表现足够优秀。(作者说的)


### 一些问题

#### 为什么B要初始化为0呢？

在使用LoRA进行微调时，选择将矩阵 B 初始化为零，而将矩阵 A 进行高斯分布的随机初始化，可能是出于以下几个方面的考虑：

1. **简化模型参数空间：** 将矩阵 B 初始化为零可以简化模型的参数空间，并且在开始微调时不会对模型产生过多的影响。这样做可以使得模型更容易收敛到一个合适的解。

2. **控制参数数量：** 将矩阵 B 初始化为零可以有效地减少模型参数的数量，从而降低计算成本和内存占用。这在资源受限的情况下尤其有用。

3. **避免过拟合：** 零初始化可以减少过拟合的风险。如果矩阵 B 的参数过多且过大，可能会导致模型在微调时对训练数据过度拟合，而零初始化可以一定程度上减轻这种情况。

4. **降低初始误差：** 零初始化可以避免在微调初始阶段引入额外的误差。如果 B 初始化为非零值，可能会引入一定程度的初始偏差，从而影响模型的收敛速度和性能。

总的来说，将矩阵 B 初始化为零可能是为了简化模型、降低计算成本、减少过拟合的风险以及避免初始误差等方面考虑的结果。