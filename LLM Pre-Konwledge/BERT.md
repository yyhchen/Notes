# 重新复习 BERT 的一些收获

---

### BERT中 [CLS] 的作用
在BERT模型中，CLS（Classification）是一个特殊的标记，通常用来表示整个输入序列的分类或汇总信息。BERT的输入格式是在句子的开头添加一个特殊的[CLS]标记，表示整个序列的语义。对应的，BERT模型的输出中，[CLS]标记对应的隐藏状态被用作整个序列的表示。

在BERT模型中，任务的分类或汇总可以基于[CLS]标记的隐藏状态进行。例如，在进行文本分类任务时，可以使用[CLS]标记对应的隐藏状态来作为整个句子的表示，然后将其输入到分类层进行具体的分类。

<br>
<br>
<br>

### 为什么 [CLS] 会起作用

BERT中的[CLS]标记在预训练阶段的任务中具有特殊的角色，它被用作整个句子的表示。这是因为BERT在预训练时采用了两个主要的自监督学习任务：

1. **Masked Language Model (MLM)：** 在这个任务中，模型在输入序列中随机掩盖一些词，然后尝试预测被掩盖的词。[CLS]标记的信息用于整合整个句子的上下文信息，以便在预测被掩盖词时能够考虑全局语境。

2. **Next Sentence Prediction (NSP)：** 模型接受两个句子的输入，然后预测这两个句子是否是相邻的。[CLS]标记的隐藏状态被用作整个句子对的表示。在这个任务中，[CLS]需要能够捕获两个句子之间的语义关系，以便正确预测它们是否相邻。

由于这两个任务的存在，[CLS]标记在预训练过程中学到了对整个句子语义信息的强大表示。当在下游任务中使用BERT进行微调时，可以使用[CLS]标记对应的隐藏状态作为整个句子的表示，因为它已经在预训练阶段学到了适应多种语义任务的能力。

需要注意的是，**[CLS]标记在预训练期间的位置固定，但其表示是通过模型在上下文中学到的，因此它能够捕获整个句子的语义信息**。这就是为什么[CLS]标记通常被用来表示整个句子的原因。